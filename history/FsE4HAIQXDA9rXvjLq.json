[{
  "history_id" : "8dkus4c0s43",
  "history_input" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\nemissions = pd.read_csv('https://raw.githubusercontent.com/ZihengSun/EmissionAI/main/data/tropomi_epa_kvps_NO2_2019_56.csv' , parse_dates=[\"Date\"])\nprint(emissions)\ndemo_dir = f\"{home}/geoweaver_demo/\"\nif not os.path.exists(demo_dir):\n\tos.mkdir(demo_dir)\nemissions.to_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")",
  "history_output" : "     FID   Latitude  ...  Precip (Monthly) Cloud Fraction (Monthly)\n0     56  31.488019  ...          0.000053                 0.470510\n1     56  31.488019  ...          0.000053                 0.470510\n2     56  31.488019  ...          0.000053                 0.470510\n3     56  31.488019  ...          0.000053                 0.470510\n4     56  31.488019  ...          0.000053                 0.470510\n..   ...        ...  ...               ...                      ...\n162   56  31.488019  ...          0.000052                 0.548777\n163   56  31.488019  ...          0.000052                 0.548777\n164   56  31.488019  ...          0.000052                 0.548777\n165   56  31.488019  ...          0.000052                 0.548777\n166   56  31.488019  ...          0.000052                 0.548777\n[167 rows x 10 columns]\n",
  "history_begin_time" : 1649179858097,
  "history_end_time" : 1649179871608,
  "history_notes" : null,
  "history_process" : "aowlun",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wikdqqat3pb",
  "history_input" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\nemissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")\nprint(\"==================>\")\nprint(\"Describe data\")\nprint(emissions.describe())\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Check shape of data\")\nprint(emissions.shape)\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Check available columns\")\nprint(emissions.columns)\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Any NULL values in features?\")\nprint(emissions.isnull().sum())\nprint(\"==================>\")",
  "history_output" : "==================>\nDescribe data\n       Unnamed: 0    FID  ...  Precip (Monthly)  Cloud Fraction (Monthly)\ncount   167.00000  167.0  ...        167.000000                167.000000\nmean     83.00000   56.0  ...          0.000052                  0.509011\nstd      48.35287    0.0  ...          0.000026                  0.048822\nmin       0.00000   56.0  ...          0.000004                  0.359887\n25%      41.50000   56.0  ...          0.000046                  0.486571\n50%      83.00000   56.0  ...          0.000047                  0.525576\n75%     124.50000   56.0  ...          0.000074                  0.526714\nmax     166.00000   56.0  ...          0.000098                  0.691063\n[8 rows x 10 columns]\n==================>\n==================>\nCheck shape of data\n(167, 11)\n==================>\n==================>\nCheck available columns\nIndex(['Unnamed: 0', 'FID', 'Latitude', 'Longitude', 'Date', 'EPA_NO2/100000',\n       'TROPOMI*1000', 'Wind (Monthly)', 'Temp (Monthly)', 'Precip (Monthly)',\n       'Cloud Fraction (Monthly)'],\n      dtype='object')\n==================>\n==================>\nAny NULL values in features?\nUnnamed: 0                  0\nFID                         0\nLatitude                    0\nLongitude                   0\nDate                        0\nEPA_NO2/100000              0\nTROPOMI*1000                0\nWind (Monthly)              0\nTemp (Monthly)              0\nPrecip (Monthly)            0\nCloud Fraction (Monthly)    0\ndtype: int64\n==================>\n",
  "history_begin_time" : 1649179873152,
  "history_end_time" : 1649179873870,
  "history_notes" : null,
  "history_process" : "nj80ks",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0c1nsh2mmkm",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\nplt.style.use('fivethirtyeight')\nemissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10,8))  # 1 row, 2 columns\n\nemissions.plot(x='EPA_NO2/100000', y='TROPOMI*1000', kind='scatter', color='orange', ax=ax1)\n\nemissions.plot(x='Wind (Monthly)', y='Cloud Fraction (Monthly)', kind='scatter', color='green', ax=ax2)\n\nax1.set_xlabel('EPA_NO2/100000',fontsize=15)\nax2.set_xlabel('Wind (Monthly)',fontsize=15)\nax1.set_ylabel('TROPOMI*1000',fontsize=15)\nax2.set_ylabel('Cloud Fraction (Monthly)',fontsize=15)\n\nemissions.plot(x='Temp (Monthly)', y='EPA_NO2/100000', kind='scatter', color='blue', ax=ax3)\n\nemissions.plot(x='EPA_NO2/100000', y='Cloud Fraction (Monthly)', kind='scatter', color='red', ax=ax4)\n\nax3.set_xlabel('Temp (Monthly)',fontsize=15)\nax4.set_xlabel('EPA_NO2/100000',fontsize=15)\nax3.set_ylabel('TROPOMI*1000',fontsize=15)\nax4.set_ylabel('Cloud Fraction (Monthly)',fontsize=15)\n\nplt.savefig(f'{home}/geoweaver_demo/features.png')\n",
  "history_output" : "",
  "history_begin_time" : 1649179875641,
  "history_end_time" : 1649179880373,
  "history_notes" : null,
  "history_process" : "iihen4",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0v5hv5b6mvt",
  "history_input" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\n\nregression_emissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\",parse_dates=[\"Date\"])\n\nregression_emissions['dayofyear'] = regression_emissions['Date'].dt.dayofyear\nregression_emissions['dayofweek'] = regression_emissions['Date'].dt.dayofweek\nregression_emissions['dayofmonth'] = regression_emissions['Date'].dt.day\nregression_emissions = regression_emissions.drop(columns=[\"Date\"])\n\n\nregression_emissions.to_csv(f'{home}/geoweaver_demo/preprocessed.csv')",
  "history_output" : "",
  "history_begin_time" : 1649179881911,
  "history_end_time" : 1649179882551,
  "history_notes" : null,
  "history_process" : "ypwf9s",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6183w0hb967",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport tensorflow as tf\nimport math\nimport os\n\nhomedir = os.path.expanduser('~')\n\n\nplt.style.use('fivethirtyeight')\nemissions = pd.read_csv(f\"{homedir}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\",parse_dates=[\"Date\"])\n\ndef showAccuracyMetrics(mlmethod, model, y_test, y_pred):    \n\tprint(\"Model \", mlmethod, \" Performance:\")\n\t#print(y_test.shape, y_pred.shape)\t\n\tmae = metrics.mean_absolute_error(y_test, \t\t\ty_pred)\n\tmse = metrics.mean_squared_error(y_test, \t\t\ty_pred)\n\tr2 = metrics.r2_score(y_test, y_pred)\n\tprint(\"   MAE: \", mae)\n\tprint(\"   MSE: \", mse)    \n\tprint(\"   R2: \", r2)\n    \nemissions['dayofyear'] = emissions['Date'].dt.dayofyear\nemissions['dayofweek'] = emissions['Date'].dt.dayofweek\nemissions['dayofmonth'] = emissions['Date'].dt.day\nemissions = emissions.drop(columns=[\"Date\"])\n\n# Separating dependednt & Indepented Variables \nx = emissions.iloc[:, emissions.columns != 'EPA_NO2/100000'].values\ny = emissions.iloc[:,  emissions.columns == 'EPA_NO2/100000']\n\n# show the shape of x and y to make sure they have the same length\n\n# Train Test Split at ratio 0.33\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\n# Define Keras NN model\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=12, activation='relu'))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(optimizer=\"adadelta\", loss=\"mse\",  metrics=[tf.keras.metrics.mean_squared_error, 'accuracy'])\nhistory = model.fit(x_train, y_train, batch_size=8, validation_split = 0.2, epochs=50)\ny_test_pred = model.predict(x_test)\n\nshowAccuracyMetrics(\"Neural Network: \", model, y_test, y_test_pred)\n# \\\"Loss\\\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig(f'{homedir}/geoweaver_demo/NN_modelLoss.png')\n\ndef visualizeResults(modelname, x_test, y_test, pred):\n\t# Visualization\n    ## Check the fitting on training set\n\tplt.scatter(x_test[:,3], y_test, color='blue')\n\tplt.scatter(x_test[:,3], pred, color='black')\n\t#plt.scatter(y_test, pred, color='black')\n\tplt.title(modelname + ' Fit on testing set')\n\tplt.xlabel('TROMPOMI-Test')\n\tplt.ylabel('EPA-Test')\n\tplt.show()\n    \n#visualizeResults(\"Neural Network\", x_test, y_test, y_test_pred)",
  "history_output" : "",
  "history_begin_time" : 1649179884425,
  "history_end_time" : 1649179888219,
  "history_notes" : null,
  "history_process" : "3d9c6b",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mngnjmyvyhu",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport tensorflow as tf\nimport math\n\n\ndef showAccuracyMetrics(mlmethod, model, y_test, y_pred):    \n\tprint(\"Model \", mlmethod, \" Performance:\")\n\t#print(y_test.shape, y_pred.shape)\t\n\tmae = metrics.mean_absolute_error(y_test, \t\t\ty_pred)\n\tmse = metrics.mean_squared_error(y_test, \t\t\ty_pred)\n\tr2 = metrics.r2_score(y_test, y_pred)\n\tprint(\"   MAE: \", mae)\n\tprint(\"   MSE: \", mse)    \n\tprint(\"   R2: \", r2)\n\nplt.style.use('fivethirtyeight')\n\nemissions_alabama_all = pd.read_csv('~/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv',parse_dates=[\"Date\"])\n\nemissions_alabama_all['dayofyear'] = emissions_alabama_all['Date'].dt.dayofyear\nemissions_alabama_all['dayofweek'] = emissions_alabama_all['Date'].dt.dayofweek\nemissions_alabama_all['dayofmonth'] = emissions_alabama_all['Date'].dt.day\nemissions_alabama_all = emissions_alabama_all.drop(columns=[\"Date\"])\n\ndef create_dataset(dataset, look_back=7):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1): \n        a = dataset[i:(i+look_back), 1:] \n#         print(a)\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0]) \n    return np.array(dataX), np.array(dataY)\n    \ndataset = emissions_alabama_all.values\ndataset = dataset.astype('float32')\n\n# normalize the dataset\nlook_back = 7\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\n# reshape into X=t and Y=t+1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n\nprint(\"X_train's shape: \", trainX.shape)\nprint(\"y_train's shape: \", trainY.shape)\nprint(\"x_test's shape: \", testX.shape)\nprint(\"y_test's shape: \", testY.shape)\n\n# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 12, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 12, testX.shape[1]))\n\n# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adadelta', metrics=[tf.keras.metrics.mean_squared_error, 'accuracy'])\nhistory = model.fit(trainX, trainY, epochs=50, batch_size=1)\n\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\nshowAccuracyMetrics(\"LSTM [Alabama Plant All features]: \", model, testY, testPredict)\n\n# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\n# plot baseline and predictions\n#plt.plot(dataset[:,0])\n#plt.plot(trainPredictPlot[:,0])\n#plt.plot(testPredictPlot[:,0])\n#plt.legend([\"Data\", \"Train\", \"Test\"])\n#plt.title(\"One plant (ID 56, Alabama)\")\n#plt.savefig('/Users/uhhmed/geoweaver_demo/LSTM_model.png')\n",
  "history_output" : "2022-04-05 13:31:50.966227: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nX_train's shape:  (103, 7, 12)\ny_train's shape:  (103,)\nx_test's shape:  (48, 7, 12)\ny_test's shape:  (48,)\nTraceback (most recent call last):\n  File \"TrainModel_LSTM.py\", line 73, in <module>\n    model.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2])))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 522, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py\", line 208, in add\n    layer(x)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 660, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 945, in __call__\n    return self._functional_construction_call(inputs, args, kwargs,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1083, in _functional_construction_call\n    outputs = self._keras_tensor_symbolic_call(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 816, in _keras_tensor_symbolic_call\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 856, in _infer_output_signature\n    outputs = call_fn(inputs, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\", line 1139, in call\n    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 860, in _process_inputs\n    initial_state = self.get_initial_state(inputs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 642, in get_initial_state\n    init_state = get_initial_state_fn(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 2508, in get_initial_state\n    return list(_generate_zero_filled_state_for_cell(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 2990, in _generate_zero_filled_state_for_cell\n    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 3006, in _generate_zero_filled_state\n    return tf.nest.map_structure(create_zeros, state_size)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 867, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 867, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\", line 3003, in create_zeros\n    return tf.zeros(init_state_size, dtype=dtype)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2911, in wrapped\n    tensor = fun(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2960, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2896, in _constant_if_small\n    if np.prod(shape) < 1000:\n  File \"<__array_function__ internals>\", line 180, in prod\n  File \"/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 3088, in prod\n    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 867, in __array__\n    raise NotImplementedError(\nNotImplementedError: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
  "history_begin_time" : 1649179888593,
  "history_end_time" : 1649179911493,
  "history_notes" : null,
  "history_process" : "w66uu5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "jt05zcqlc5g",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.svm import SVR\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\ndef showAccuracyMetrics(mlmethod, model, y_test, y_pred):    \n\tprint(\"Model \", mlmethod, \" Performance:\")\n\t#print(y_test.shape, y_pred.shape)\t\n\tmae = metrics.mean_absolute_error(y_test, \t\t\ty_pred)\n\tmse = metrics.mean_squared_error(y_test, \t\t\ty_pred)\n\tr2 = metrics.r2_score(y_test, y_pred)\n\tprint(\"   MAE: \", mae)\n\tprint(\"   MSE: \", mse)    \n\tprint(\"   R2: \", r2)\n\nregression_emissions = pd.read_csv(f'{home}/geoweaver_demo/preprocessed.csv')\n\nX = regression_emissions[['dayofyear']]\ny = regression_emissions['EPA_NO2/100000']\nxtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=0.30, random_state=42)\n\nxtrain = X.iloc[:116]\nytrain = y.iloc[:116]\nxtest = X.iloc[116:]\nytest = y.iloc[116:]\n\nsvr_rbf = SVR(kernel='rbf', C=1e4, gamma=0.35)\nsvr_rbf.fit(X, y)\ny_rbf = svr_rbf.predict(X)\n\nshowAccuracyMetrics(\"SVR: \", svr_rbf, y, y_rbf)\n\nplt.scatter(X, y, c='k', label='data')\nplt.plot(X, y_rbf, c='g', label='RBF model')\nplt.xlabel('dayofyear')\nplt.ylabel('EPA_NO2/100000')\nplt.title('Support Vector Regression')\nplt.legend()\nplt.savefig(f'{home}/geoweaver_demo/SVR_model.png')",
  "history_output" : "Model  SVR:   Performance:\n   MAE:  0.051701172004056876\n   MSE:  0.0036857502571577103\n   R2:  0.5958888534234995\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 142, in pandas._libs.index.IndexEngine.get_loc\nTypeError: '(slice(None, None, None), None)' is an invalid key\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"SVR Model.py\", line 40, in <module>\n    plt.plot(X, y_rbf, c='g', label='RBF model')\n  File \"/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 2757, in plot\n    return gca().plot(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\", line 1632, in plot\n    lines = [*self._get_lines(*args, data=data, **kwargs)]\n  File \"/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\", line 312, in __call__\n    yield from self._plot_args(this, kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\", line 487, in _plot_args\n    x = _check_1d(xy[0])\n  File \"/opt/anaconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\", line 1327, in _check_1d\n    ndim = x[:, None].ndim\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3628, in get_loc\n    self._check_indexing_error(key)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 5637, in _check_indexing_error\n    raise InvalidIndexError(key)\npandas.errors.InvalidIndexError: (slice(None, None, None), None)\n",
  "history_begin_time" : 1649179884198,
  "history_end_time" : 1649179888198,
  "history_notes" : null,
  "history_process" : "g7gk7m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mhhiwfci75p",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\ndef showAccuracyMetrics(mlmethod, model, y_test, y_pred):    \n\tprint(\"Model \", mlmethod, \" Performance:\")\n\t#print(y_test.shape, y_pred.shape)\t\n\tmae = metrics.mean_absolute_error(y_test, \t\t\ty_pred)\n\tmse = metrics.mean_squared_error(y_test, \t\t\ty_pred)\n\tr2 = metrics.r2_score(y_test, y_pred)\n\tprint(\"   MAE: \", mae)\n\tprint(\"   MSE: \", mse)    \n\tprint(\"   R2: \", r2)\n\nregression_emissions = pd.read_csv(f'{home}/geoweaver_demo/preprocessed.csv')\n    \ntarget_column = ['EPA_NO2/100000'] \npredictors = ['TROPOMI*1000', 'dayofyear', 'dayofweek', 'dayofmonth']\n\nall_X = regression_emissions[predictors]\nall_y = regression_emissions[target_column]\n\n\nxtrain, xtest, ytrain, ytest = train_test_split(all_X,all_y,test_size=0.30, random_state=42)\n\n\nrandomForestregModel = RandomForestRegressor(max_depth=15)\nrandomForestregModel.fit(xtrain, np.ravel(ytrain))\n\nypred = randomForestregModel.predict(xtest)\n\n\nshowAccuracyMetrics(\"RF: \", randomForestregModel, ytest, ypred)\n\n\n\nfn=all_X.columns\ncn=all_y.columns\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (0.5,0.5), dpi=800)\nout = tree.plot_tree(randomForestregModel.estimators_[0],\n               feature_names = fn, \n#                class_names=cn,\n               filled = True,\n               );\n\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(.1)\n        \nplt.savefig(f'{home}/geoweaver_demo/tree.eps',format='eps',bbox_inches = \"tight\")",
  "history_output" : "Model  RF:   Performance:\n   MAE:  0.059754039915966416\n   MSE:  0.006143582037036858\n   R2:  0.26268511140781403\n",
  "history_begin_time" : 1649179889238,
  "history_end_time" : 1649179901578,
  "history_notes" : null,
  "history_process" : "jl0rv8",
  "host_id" : "100001",
  "indicator" : "Done"
}]
