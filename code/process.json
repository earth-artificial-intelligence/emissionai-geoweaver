[{
  "id" : "nj80ks",
  "name" : "ExploreDataProperties",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\nemissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")\nprint(\"==================>\")\nprint(\"Describe data\")\nprint(emissions.describe())\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Check shape of data\")\nprint(emissions.shape)\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Check available columns\")\nprint(emissions.columns)\nprint(\"==================>\")\nprint(\"==================>\")\nprint(\"Any NULL values in features?\")\nprint(emissions.isnull().sum())\nprint(\"==================>\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "iihen4",
  "name" : "VisualizeFeatures",
  "description" : null,
  "code" : "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\nplt.style.use('fivethirtyeight')\nemissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10,8))  # 1 row, 2 columns\n\nemissions.plot(x='EPA_NO2/100000', y='TROPOMI*1000', kind='scatter', color='orange', ax=ax1)\n\nemissions.plot(x='Wind (Monthly)', y='Cloud Fraction (Monthly)', kind='scatter', color='green', ax=ax2)\n\nax1.set_xlabel('EPA_NO2/100000',fontsize=15)\nax2.set_xlabel('Wind (Monthly)',fontsize=15)\nax1.set_ylabel('TROPOMI*1000',fontsize=15)\nax2.set_ylabel('Cloud Fraction (Monthly)',fontsize=15)\n\nemissions.plot(x='Temp (Monthly)', y='EPA_NO2/100000', kind='scatter', color='blue', ax=ax3)\n\nemissions.plot(x='EPA_NO2/100000', y='Cloud Fraction (Monthly)', kind='scatter', color='red', ax=ax4)\n\nax3.set_xlabel('Temp (Monthly)',fontsize=15)\nax4.set_xlabel('EPA_NO2/100000',fontsize=15)\nax3.set_ylabel('TROPOMI*1000',fontsize=15)\nax4.set_ylabel('Cloud Fraction (Monthly)',fontsize=15)\n\nplt.savefig(f'{home}/geoweaver_demo/features.png')\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ypwf9s",
  "name" : "Data Preprocessing",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\n\nregression_emissions = pd.read_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\",parse_dates=[\"Date\"])\n\nregression_emissions['dayofyear'] = regression_emissions['Date'].dt.dayofyear\nregression_emissions['dayofweek'] = regression_emissions['Date'].dt.dayofweek\nregression_emissions['dayofmonth'] = regression_emissions['Date'].dt.day\nregression_emissions = regression_emissions.drop(columns=[\"Date\"])\n\n\nregression_emissions.to_csv(f'{home}/geoweaver_demo/preprocessed.csv')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "g7gk7m",
  "name" : "SVR Model",
  "description" : null,
  "code" : "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.svm import SVR\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\n\ndef showAccuracyMetrics(mlmethod, model, y_test, y_pred):    \n\tprint(\"Model \", mlmethod, \" Performance:\")\n\t#print(y_test.shape, y_pred.shape)\t\n\tmae = metrics.mean_absolute_error(y_test, \t\t\ty_pred)\n\tmse = metrics.mean_squared_error(y_test, \t\t\ty_pred)\n\tr2 = metrics.r2_score(y_test, y_pred)\n\tprint(\"   MAE: \", mae)\n\tprint(\"   MSE: \", mse)    \n\tprint(\"   R2: \", r2)\n\nregression_emissions = pd.read_csv(f'{home}/geoweaver_demo/preprocessed.csv')\n\nX = regression_emissions[['dayofyear']]\ny = regression_emissions['EPA_NO2/100000']\nxtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=0.30, random_state=42)\n\nxtrain = X.iloc[:116]\nytrain = y.iloc[:116]\nxtest = X.iloc[116:]\nytest = y.iloc[116:]\n\nsvr_rbf = SVR(kernel='rbf', C=1e4, gamma=0.35)\nsvr_rbf.fit(X, y)\ny_rbf = svr_rbf.predict(X)\n\nshowAccuracyMetrics(\"SVR: \", svr_rbf, y, y_rbf)\n\nplt.scatter(X, y, c='k', label='data')\nplt.plot(X, y_rbf, c='g', label='RBF model')\nplt.xlabel('dayofyear')\nplt.ylabel('EPA_NO2/100000')\nplt.title('Support Vector Regression')\nplt.legend()\nplt.savefig(f'{home}/geoweaver_demo/SVR_model.png')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "kce2jy",
  "name" : "Model Evaluations",
  "description" : "python",
  "code" : "# Write first python in Geoweaver",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "aowlun",
  "name" : "MergeData",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom pathlib import Path\nhome = str(Path.home())\nemissions = pd.read_csv('https://raw.githubusercontent.com/ZihengSun/EmissionAI/main/data/tropomi_epa_kvps_NO2_2019_56.csv' , parse_dates=[\"Date\"])\nprint(emissions)\ndemo_dir = f\"{home}/geoweaver_demo/\"\nif not os.path.exists(demo_dir):\n\tos.mkdir(demo_dir)\nemissions.to_csv(f\"{home}/geoweaver_demo/tropomi_epa_kvps_NO2_2019_56.csv\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "o0lq93",
  "name" : "data_merra2_extraction",
  "description" : "python",
  "code" : "\"\"\" \nCheck if host machine contains required packages to run this process. \nIf packages are not available, this process will install them.\nThis process will get surface temperature, bias-corrected precipitation, \ncloud fraction, and surface wind speed from different MERRA-2 collections,\nresample them to daily data and save to a csv file.\nUser can specify duration of data to extract in lines (67 - 71).\nNOTE: This process also needs a NASA Earthdata account.\nPlease update line 46 with a username and password to proceed with execution.\n \"\"\"\n\nimport sys\nimport subprocess\nimport pkg_resources\n\n# Required packages to run this process.\nrequired = {'xarray', 'netCDF4', 'dask', 'pandas'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n    print(\"Packages missing and will be installed: \", missing)\n    python = sys.executable\n    subprocess.check_call(\n        [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES VALIDATION  #\n################################\n\n\n##############################################\n#  NASA Earthdata account credentials setup  #\n##############################################\n\n# get current user home directory to create necessary file for earthdata access.\nfrom os.path import expanduser\nhome_dir = expanduser(\"~\")\n\n# create .netrc file to insert earthdata username and password\nopen(home_dir + '/.netrc', 'w').close()\nopen(home_dir + '/.urs_cookies', 'w').close()\nsubprocess.check_call(\n    ['echo \"machine urs.earthdata.nasa.gov login <username> password <password>\" >> ' + home_dir + '/.netrc'], shell=True)\n\nopen(home_dir + '/.dodsrc', 'w').close()\nsubprocess.check_call(\n    ['echo \"HTTP.COOKIEJAR=' + home_dir + '/.urs_cookies\" >> ' + home_dir + '/.dodsrc'], shell=True)\nsubprocess.check_call(\n    ['echo \"HTTP.NETRC=' + home_dir + '/.netrc\" >> ' + home_dir + '/.dodsrc'], shell=True)\n\n#####################################################\n#  END OF NASA Earthdata account credentials setup  #\n#####################################################\n\n\n\"\"\" Extract MERRA-2 Hourly data for the month of January 2019 \"\"\"\nimport pandas as pd\nimport dask\nimport netCDF4\nimport xarray as xr\n\n\n# Time frame of MERRA-2 data to collect\nyear = '2019'\nmonth_begin = '01'\nmonth_end = '03'\nday_begin = '01'\nday_end = '31'\n\n# MERRA-2 M2I1NXASM collection (hourly) to get Temp and Wind variables (T2M, V2M).\ncollection_shortname = 'M2I1NXASM'\ncollection_longname = 'inst1_2d_asm_Nx'\ncollection_number = 'MERRA2_400'\nMERRA2_version = '5.12.4'\n\n\n# OPeNDAP URL\nurl = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/{}.{}/{}'.format(\n    collection_shortname, MERRA2_version, year)\nfiles_month = ['{}/{}/{}.{}.{}{}.nc4'.format(url, month_days[0:2], collection_number, collection_longname, year, month_days)\n               for month_days in pd.date_range(year + '-' + month_begin + '-' + day_begin, year + '-' + month_end + '-' + day_end, freq='D').strftime(\"%m%d\").tolist()]\n\n\n# Get the number of files\nlen_files_month = len(files_month)\n\n\nprint(\"{} files to be opened:\".format(len_files_month))\nprint(\"files_month\", files_month)\n\n# Read dataset URLs\nds = xr.open_mfdataset(files_month)\n\n\n# MERRA-2 M2T1NXLND collection (hourly) to get Total precipitation variable (PRECTOTLAND).\ncollection_shortname = 'M2T1NXLND'\ncollection_longname = 'tavg1_2d_lnd_Nx'\ncollection_number = 'MERRA2_400'\nMERRA2_version = '5.12.4'\n\n\n# OPeNDAP URL\nurl = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/{}.{}/{}'.format(\n    collection_shortname, MERRA2_version, year)\nfiles_month = ['{}/{}/{}.{}.{}{}.nc4'.format(url, month_days[0:2], collection_number, collection_longname, year, month_days)\n               for month_days in pd.date_range(year + '-' + month_begin + '-' + day_begin, year + '-' + month_end + '-' + day_end, freq='D').strftime(\"%m%d\").tolist()]\n\n# Get the number of files\nlen_files_month = len(files_month)\n\n\nprint(\"{} files to be opened:\".format(len_files_month))\nprint(\"files_month\", files_month)\n\n# Read dataset URLs\nds_precip = xr.open_mfdataset(files_month)\n\n\n# MERRA-2 M2T1NXRAD collection (hourly) to get Cloud Fraction variable (CLDTOT).\ncollection_shortname = 'M2T1NXRAD'\ncollection_longname = 'tavg1_2d_rad_Nx'\ncollection_number = 'MERRA2_400'\nMERRA2_version = '5.12.4'\n\n\n# OPeNDAP URL\nurl = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/{}.{}/{}'.format(\n    collection_shortname, MERRA2_version, year)\nfiles_month = ['{}/{}/{}.{}.{}{}.nc4'.format(url, month_days[0:2], collection_number, collection_longname, year, month_days)\n               for month_days in pd.date_range(year + '-' + month_begin + '-' + day_begin, year + '-' + month_end + '-' + day_end, freq='D').strftime(\"%m%d\").tolist()]\n\n# Get the number of files\nlen_files_month = len(files_month)\n\n\nprint(\"{} files to be opened:\".format(len_files_month))\nprint(\"files_month\", files_month)\n\n# Read dataset URLs\nds_cloud = xr.open_mfdataset(files_month)\n\n\n# extract values from all datasets based on location (Alabama plant)\nalabama_plant_temp_wind = ds.sel(lat=31.48, lon=-87.91, method='nearest')\nalabama_plant_temp_wind = alabama_plant_temp_wind[['T2M', 'V2M']]\n\nalabama_plant_precip = ds_precip.sel(lat=31.48, lon=-87.91, method='nearest')\nalabama_plant_precip = alabama_plant_precip[['PRECTOTLAND']]\n\nalabama_plant_cloud = ds_cloud.sel(lat=31.48, lon=-87.91, method='nearest')\nalabama_plant_cloud = alabama_plant_cloud[['CLDTOT']]\n\n\n# Resample all datasets by day\nalabama_plant_temp_wind_mean = alabama_plant_temp_wind.resample(\n    time=\"1D\").mean(dim='time', skipna=True)\nalabama_plant_precip_mean = alabama_plant_precip.resample(\n    time=\"1D\").mean(dim='time', skipna=True)\nalabama_plant_cloud_mean = alabama_plant_cloud.resample(\n    time=\"1D\").mean(dim='time', skipna=True)\n\n# Convert datasets to pandas df and save to CSV file.\nalabama_plant_temp_wind_mean_df = alabama_plant_temp_wind_mean.to_dataframe()\nalabama_plant_precip_mean_df = alabama_plant_precip_mean.to_dataframe()\nalabama_plant_cloud_mean_df = alabama_plant_cloud_mean.to_dataframe()\n\nmerged_dfs = alabama_plant_temp_wind_mean_df.merge(\n    alabama_plant_precip_mean_df, on='time').merge(alabama_plant_cloud_mean_df, on='time')\nmerged_dfs = merged_dfs[['T2M', 'V2M', 'PRECTOTLAND', 'CLDTOT']]\n\nmerged_dfs.to_csv(home_dir + '/merra2_daily_mean_January_2019.csv')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wji81a",
  "name" : "data_tropomi_extraction",
  "description" : "python",
  "code" : "import json\nimport pandas as pd\nimport ee\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize()\n\n# identify a 500 meter buffer around our Point Of Interest (POI)\npoi = ee.Geometry.Point(-87.910747, 31.488019).buffer(500)\n\nGet TROPOMI NRTI Image Collection for GoogleEarth Engine\ntropomiCollection = ee.ImageCollection(\"COPERNICUS/S5P\n/NRTI/L3_NO2\").filterDate('2019-01-01','2019-03-31')\n\ndef poi_mean(img):\n    # This function will reduce all the points in the area we specifed in \"poi\" \n    and average all the data into a single daily value\n    mean = img.reduceRegion(reducer=ee.Reducer.mean(), \n    geometry=poi,scale=250).get('tropospheric_NO2_column_number_density')\n    return img.set('date', img.date().format()).set('mean',mean)\n    \n# Map function to our ImageCollection\npoi_reduced_imgs = tropomiCollection.map(poi_mean)\nnested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2)['date','mean']).values().get(0)\n\n# we need to call the callback method \"getInfo\" to retrieve the data\ndf = pd.DataFrame(nested_list.getInfo(), columns=['date','tropomi_no2_mean'])\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date')\n\t# Scaling the data to later match our target feature scale\ndf['tropomi_no2_mean'] = df['tropomi_no2_mean']*1000\n# Save data to CSV file\ndf.to_csv('tropomi_no2.csv')\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6kcl4m",
  "name" : "data_MCD19A2_extraction",
  "description" : "python",
  "code" : "import json\nimport pandas as pd\nimport ee\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize()\n\n# identify a 500 meter buffer around our Point Of Interest (POI)\npoi = ee.Geometry.Point(-87.910747, 31.488019).buffer(500)\n\nmcd19a2 = ee.ImageCollection(\"MODIS/006/MCD19A2_GRANULES\").filterDate('2019-01-01','2019-03-31')\ndef poi_mean(img):\n    mean = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=250).get('Optical_Depth_047')\n    return img.set('date', img.date().format()).set('mean',mean)\n\n    \n# Map function to our ImageCollection\npoi_reduced_imgs = tropomiCollection.map(poi_mean)\nnested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2)['date','mean']).values().get(0)\n\n# we need to call the callback method \"getInfo\" to retrieve the data\ndf = pd.DataFrame(nested_list.getInfo(), columns=['date','tropomi_no2_mean'])\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date')\n\t# Scaling the data to later match our target feature scale\ndf['tropomi_no2_mean'] = df['tropomi_no2_mean']*1000\n# Save data to CSV file\ndf.to_csv('tropomi_no2.csv')\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
